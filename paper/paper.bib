@article{iclabel2019,
  title    = {ICLabel: An automated electroencephalographic independent component classifier, dataset, and website},
  journal  = {NeuroImage},
  volume   = {198},
  pages    = {181-197},
  year     = {2019},
  issn     = {1053-8119},
  doi      = {10.1016/j.neuroimage.2019.05.026},
  url      = {https://www.sciencedirect.com/science/article/pii/S1053811919304185},
  author   = {Luca Pion-Tonachini and Ken Kreutz-Delgado and Scott Makeig},
  keywords = {EEG, ICA, Classification, Crowdsourcing},
  abstract = {The electroencephalogram (EEG) provides a non-invasive, minimally restrictive, and relatively low-cost measure of mesoscale brain dynamics with high temporal resolution. Although signals recorded in parallel by multiple, near-adjacent EEG scalp electrode channels are highly-correlated and combine signals from many different sources, biological and non-biological, independent component analysis (ICA) has been shown to isolate the various source generator processes underlying those recordings. Independent components (IC) found by ICA decomposition can be manually inspected, selected, and interpreted, but doing so requires both time and practice as ICs have no order or intrinsic interpretations and therefore require further study of their properties. Alternatively, sufficiently-accurate automated IC classifiers can be used to classify ICs into broad source categories, speeding the analysis of EEG studies with many subjects and enabling the use of ICA decomposition in near-real-time applications. While many such classifiers have been proposed recently, this work presents the ICLabel project comprised of (1) the ICLabel dataset containing spatiotemporal measures for over 200,000 ICs from more than 6000 EEG recordings and matching component labels for over 6000 of those ICs, all using common average reference, (2) the ICLabel website for collecting crowdsourced IC labels and educating EEG researchers and practitioners about IC interpretation, and (3) the automated ICLabel classifier, freely available for MATLAB. The ICLabel classifier improves upon existing methods in two ways: by improving the accuracy of the computed label estimates and by enhancing its computational efficiency. The classifier outperforms or performs comparably to the previous best publicly available automated IC component classification method for all measured IC categories while computing those labels ten times faster than that classifier as shown by a systematic comparison against other publicly available EEG IC classifiers.}
}

@article{Agramfort2013,
  title    = {MEG and EEG data analysis with MNE-Python},
  author   = {Gramfort, Alexandre and
              Luessi, Martin and
              Larson, Eric and
              Engemann, Denis A and
              Strohmeier, Daniel and
              Brodbeck, Christian and
              Goj, Roman and
              Jas, Mainak and
              Brooks, Teon and
              Parkkonen, Lauri and
              Hämäläinen, Matti},
  abstract = {Magnetoencephalography and electroencephalography (M/EEG) measure
              the weak electromagnetic signals generated by neuronal activity
              in the brain. Using these signals to characterize and locate
              neural activation in the brain is a challenge that requires
              expertise in physics, signal processing, statistics, and
              numerical methods. As part of the MNE software suite, MNE-Python
              is an open-source software package that addresses this challenge
              by providing state-of-the-art algorithms implemented in Python
              that cover multiple methods of data preprocessing,
              source localization, statistical analysis, and estimation of
              functional connectivity between distributed brain regions. All
              algorithms and utility functions are implemented in a consistent
              manner with well-documented interfaces, enabling users to create
              M/EEG data analysis pipelines by writing Python scripts.
              Moreover, MNE-Python is tightly integrated with the core Python
              libraries for scientific comptutation (NumPy, SciPy) and
              visualization (matplotlib and Mayavi), as well as the greater
              neuroimaging ecosystem in Python via the Nibabel package. The
              code is provided under the new BSD license allowing code reuse,
              even in commercial products. Although MNE-Python has only been
              under heavy development for a couple of years, it has rapidly
              evolved with expanded analysis capabilities and pedagogical
              tutorials because multiple labs have collaborated during code
              development to help share best practices. MNE-Python also gives
              easy access to preprocessed datasets, helping users to get
              started quickly and facilitating reproducibility of methods
              by other researchers. Full documentation, including dozens of
              examples, is available at http://martinos.org/mne.},
  journal  = {Frontiers in Neuroscience},
  volume   = 7,
  pages    = {267},
  month    = dec,
  year     = 2013,
  doi      = {10.3389/fnins.2013.00267},
  language = {en}
}

@inproceedings{Tensorflow2016,
  title     = {TensorFlow: A system for large-scale machine learning},
  author    = {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  year      = {2016},
  url       = {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
  booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
  pages     = {265--283}
}

@article{Pytorch2019,
  author     = {Adam Paszke and
                Sam Gross and
                Francisco Massa and
                Adam Lerer and
                James Bradbury and
                Gregory Chanan and
                Trevor Killeen and
                Zeming Lin and
                Natalia Gimelshein and
                Luca Antiga and
                Alban Desmaison and
                Andreas K{\"{o}}pf and
                Edward Z. Yang and
                Zach DeVito and
                Martin Raison and
                Alykhan Tejani and
                Sasank Chilamkurthy and
                Benoit Steiner and
                Lu Fang and
                Junjie Bai and
                Soumith Chintala},
  title      = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  journal    = {CoRR},
  volume     = {abs/1912.01703},
  year       = {2019},
  url        = {http://arxiv.org/abs/1912.01703},
  eprinttype = {arXiv},
  eprint     = {1912.01703},
  timestamp  = {Tue, 02 Nov 2021 15:18:32 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1912-01703.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  doi        = {10.48550/ARXIV.1912.01703}
}
@article{Bell1995,
  author   = {Bell, Anthony J. and Sejnowski, Terrence J.},
  title    = {{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}},
  journal  = {Neural Computation},
  volume   = {7},
  number   = {6},
  pages    = {1129-1159},
  year     = {1995},
  month    = {11},
  abstract = {{We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in "blind" signal processing.}},
  issn     = {0899-7667},
  doi      = {10.1162/neco.1995.7.6.1129},
  url      = {https://doi.org/10.1162/neco.1995.7.6.1129},
  eprint   = {https://direct.mit.edu/neco/article-pdf/7/6/1129/813064/neco.1995.7.6.1129.pdf}
}



